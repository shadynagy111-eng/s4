{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadynagy111-eng/s4/blob/main/X_LSTM_Chest_Sinogram_classification_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DoBwDvJJ3l7e",
        "outputId": "7e1fa153-2f0c-484d-8fad-35678fe5546b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlstm\n",
            "  Downloading xlstm-2.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from xlstm) (2.5.1+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xlstm) (1.26.4)\n",
            "Collecting omegaconf (from xlstm)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from xlstm) (3.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.48.3)\n",
            "Collecting reportlab (from xlstm)\n",
            "  Downloading reportlab-4.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting joypy (from xlstm)\n",
            "  Downloading joypy-0.2.6-py2.py3-none-any.whl.metadata (812 bytes)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from xlstm) (6.17.1)\n",
            "Collecting dacite (from xlstm)\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting ftfy (from xlstm)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting ninja (from xlstm)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.28.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from xlstm) (13.9.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from xlstm) (4.67.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from xlstm) (0.13.2)\n",
            "Collecting mlstm_kernels (from xlstm)\n",
            "  Downloading mlstm_kernels-1.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->xlstm) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->xlstm) (4.12.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->xlstm) (5.7.1)\n",
            "Requirement already satisfied: scipy>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (1.14.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from joypy->xlstm) (3.10.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->xlstm)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (11.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab->xlstm) (5.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->xlstm) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->xlstm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->xlstm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->xlstm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->xlstm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->xlstm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->xlstm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->xlstm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->xlstm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->xlstm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->xlstm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->xlstm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->xlstm) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->xlstm) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->xlstm) (0.5.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->xlstm)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->xlstm) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->xlstm) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->xlstm) (2.8.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->xlstm) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->joypy->xlstm) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->joypy->xlstm) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->joypy->xlstm) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->xlstm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->xlstm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->xlstm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->xlstm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->xlstm) (2025.1.31)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->xlstm) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->xlstm) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->xlstm) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->xlstm) (1.17.0)\n",
            "Downloading xlstm-2.0.2-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joypy-0.2.6-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading mlstm_kernels-1.0.3-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reportlab-4.3.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=0a08413e12557d351ff1b00e6d4db87a4412ed67dce5c94feb91095fe48f71fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, reportlab, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, jedi, ftfy, dacite, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, joypy, mlstm_kernels, xlstm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 dacite-1.9.2 ftfy-6.3.1 jedi-0.19.2 joypy-0.2.6 mlstm_kernels-1.0.3 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 reportlab-4.3.1 xlstm-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "3f8c34078827466fbd2198be91cb58ab"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install xlstm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xlstm"
      ],
      "metadata": {
        "id": "Pw03fbgX44Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from xlstm import (\n",
        "    xLSTMBlockStack,\n",
        "    xLSTMBlockStackConfig,\n",
        "    mLSTMBlockConfig,\n",
        "    mLSTMLayerConfig,\n",
        "    sLSTMBlockConfig,\n",
        "    sLSTMLayerConfig,\n",
        "    FeedForwardConfig,\n",
        ")\n",
        "\n",
        "cfg = xLSTMBlockStackConfig(\n",
        "    mlstm_block=mLSTMBlockConfig(\n",
        "        mlstm=mLSTMLayerConfig(\n",
        "            conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n",
        "        )\n",
        "    ),\n",
        "    slstm_block=sLSTMBlockConfig(\n",
        "        slstm=sLSTMLayerConfig(\n",
        "            backend=\"cuda\",\n",
        "            num_heads=4,\n",
        "            conv1d_kernel_size=4,\n",
        "            bias_init=\"powerlaw_blockdependent\",\n",
        "        ),\n",
        "        feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n",
        "    ),\n",
        "    context_length=256,\n",
        "    num_blocks=4,\n",
        "    embedding_dim=128,\n",
        "    slstm_at=[1],\n",
        "\n",
        ")\n",
        "\n",
        "xlstm_stack = xLSTMBlockStack(cfg)\n",
        "\n",
        "x = torch.randn(4, 256, 128).to(\"cuda\")\n",
        "xlstm_stack = xlstm_stack.to(\"cuda\")\n",
        "y = xlstm_stack(x)\n",
        "y.shape == x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yO9HS_M3LlM",
        "outputId": "655b1021-6fbe-451b-9255-f5a9da7ebd4b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py311_cu124/slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n",
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqKDZWR5yBe",
        "outputId": "07132159-0731-445b-f856-3faca6dae622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from skimage.transform import resize,radon\n",
        "from skimage.io import imread\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "def jpg_to_sinogram(jpg_path):\n",
        "    \"\"\"\n",
        "    Loads a JPG image, converts it to a sinogram, and returns the sinogram as a NumPy array.\n",
        "    \"\"\"\n",
        "    # Load the JPG image using Pillow library\n",
        "    image = Image.open(jpg_path).convert('L')  # Convert to grayscale\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Resize the image to 64x64\n",
        "    image = resize(image, (128, 128))\n",
        "\n",
        "    # Calculate the sinogram using radon transform\n",
        "    theta = np.linspace(0., 180., max(image.shape), endpoint=False)\n",
        "    sinogram = radon(image, theta=theta, circle=True)\n",
        "\n",
        "    return sinogram"
      ],
      "metadata": {
        "id": "2dKGsPDu5Cda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the three folders containing JPG images\n",
        "folder_paths = [\"/content/drive/MyDrive/PHD/Simulation/CHEST_CT_Data/train/normal\",\n",
        "                \"/content/drive/MyDrive/PHD/Simulation/CHEST_CT_Data/train/adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\",\n",
        "                \"/content/drive/MyDrive/PHD/Simulation/CHEST_CT_Data/train/large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\",\n",
        "                \"/content/drive/MyDrive/PHD/Simulation/CHEST_CT_Data/train/squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa\"]\n",
        "\n",
        "# Create an empty list to store sinograms\n",
        "sinograms = []\n",
        "\n",
        "labels = []\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    \"normal\": 0,\n",
        "    \"adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\": 1,\n",
        "    \"large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\": 2,\n",
        "    'squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa':3,\n",
        "}\n",
        "\n",
        "# Iterate over the folders and their contents\n",
        "for folder_path in folder_paths:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\"):  # Change to .jpg\n",
        "            jpg_path = os.path.join(folder_path, filename)\n",
        "            sinogram = jpg_to_sinogram(jpg_path)\n",
        "            sinograms.append(sinogram)\n",
        "            # Extract class label from folder name\n",
        "            label = class_labels[os.path.basename(folder_path)]\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert the list of sinograms to a PyTorch tensor\n",
        "sinogram_tensor = torch.tensor(np.stack(sinograms))\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "# Print the shape of the resulting tensor\n",
        "print(\"Sinogram tensor shape:\", sinogram_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzSSNeSi0YMJ",
        "outputId": "ad187915-ef6d-440f-fe74-19dc204d8715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/skimage/transform/radon_transform.py:74: UserWarning: Radon transform: image must be zero outside the reconstruction circle\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sinogram tensor shape: torch.Size([888, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
        "\n",
        "\n",
        "# Generate random dataset\n",
        "sinogram_tensor = sinogram_tensor.type(torch.float32)\n",
        "num_samples = sinogram_tensor.shape[0]*sinogram_tensor.shape[1]   # 259\n",
        "sequence_length = sinogram_tensor.shape[2]\n",
        "\n",
        "input_size = 1\n",
        "num_classes = 4\n",
        "\n",
        "\n",
        "'''\n",
        "X = torch.randn(num_samples, sequence_length, input_size)\n",
        "y = torch.randint(0, num_classes, (num_samples,))\n",
        "'''\n",
        "# Create dataset and dataloader\n",
        "X= sinogram_tensor.reshape(num_samples, sequence_length, input_size)\n",
        "print(X.shape)\n",
        "y = labels_tensor\n",
        "y=y.repeat_interleave(sequence_length)\n",
        "print(y.shape)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.7* len(dataset))\n",
        "val_size = int(0.5*(len(dataset) - train_size))\n",
        "test_size=int(len(dataset) - train_size - val_size)\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size,test_size])\n",
        "\n",
        "# Create data loaders for training and validation sets\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader= DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "1JIsmSHi8der",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67c5bdb-4019-42b7-8fec-883f1226cde9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([113664, 128, 1])\n",
            "torch.Size([113664])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n",
        "\n",
        "# Define the model architecture\n",
        "class SinogramClassifier(nn.Module):\n",
        "    def __init__(self, xlstm_config, num_classes):\n",
        "        super(SinogramClassifier, self).__init__()\n",
        "        self.xlstm = xLSTMBlockStack(xlstm_config)\n",
        "        self.fc = nn.Linear(xlstm_config.embedding_dim, num_classes)  # Adjust output dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input to match xLSTM expected dimensions\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, embedding_dim)\n",
        "        x = self.xlstm(x)\n",
        "        x = x[:, -1, :]  # Take the output from the last timestep\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Define xLSTM configuration (you can adjust these parameters)\n",
        "xlstm_config = xLSTMBlockStackConfig(\n",
        "    mlstm_block=mLSTMBlockConfig(\n",
        "        mlstm=mLSTMLayerConfig(\n",
        "            conv1d_kernel_size=4, qkv_proj_blocksize=4, num_heads=4\n",
        "        )\n",
        "    ),\n",
        "    slstm_block=sLSTMBlockConfig(\n",
        "        slstm=sLSTMLayerConfig(\n",
        "            backend=\"cuda\",\n",
        "            num_heads=1,\n",
        "            conv1d_kernel_size=4,\n",
        "            bias_init=\"powerlaw_blockdependent\",\n",
        "        ),\n",
        "        feedforward=FeedForwardConfig(proj_factor=1.3, act_fn=\"gelu\"),\n",
        "    ),\n",
        "    context_length=128,\n",
        "    num_blocks=7,\n",
        "    embedding_dim=128,\n",
        "    slstm_at=[1],\n",
        ")\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "num_classes = 4 # Assuming 'unique_labels' is defined\n",
        "model = SinogramClassifier(xlstm_config, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)  # Adjust learning rate\n",
        "\n",
        "# Move model and data to the appropriate device (e.g., CUDA)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ai8WZa5A0f",
        "outputId": "4f2f5b59-56ab-497c-d86d-c8dd0e23c930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/usr/local/cuda/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=1', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=1', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py311_cu124/slstm_HS128BS8NH1NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/slstm_HS128BS8NH1NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module slstm_HS128BS8NH1NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module slstm_HS128BS8NH1NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n",
            "/usr/local/lib/python3.11/dist-packages/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @conditional_decorator(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SinogramClassifier(\n",
              "  (xlstm): xLSTMBlockStack(\n",
              "    (blocks): ModuleList(\n",
              "      (0): mLSTMBlock(\n",
              "        (xlstm_norm): LayerNorm()\n",
              "        (xlstm): mLSTMLayer(\n",
              "          (proj_up): Linear(in_features=128, out_features=512, bias=False)\n",
              "          (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (conv1d): CausalConv1d(\n",
              "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
              "          )\n",
              "          (conv_act_fn): SiLU()\n",
              "          (mlstm_cell): mLSTMCell(\n",
              "            (igate): Linear(in_features=768, out_features=4, bias=True)\n",
              "            (fgate): Linear(in_features=768, out_features=4, bias=True)\n",
              "            (outnorm): MultiHeadLayerNorm()\n",
              "          )\n",
              "          (ogate_act_fn): SiLU()\n",
              "          (proj_down): Linear(in_features=256, out_features=128, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): sLSTMBlock(\n",
              "        (xlstm_norm): LayerNorm()\n",
              "        (xlstm): sLSTMLayer(\n",
              "          (conv1d): CausalConv1d(\n",
              "            (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
              "          )\n",
              "          (conv_act_fn): SiLU()\n",
              "          (fgate): LinearHeadwiseExpand(in_features=128, num_heads=1, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (igate): LinearHeadwiseExpand(in_features=128, num_heads=1, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (zgate): LinearHeadwiseExpand(in_features=128, num_heads=1, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (ogate): LinearHeadwiseExpand(in_features=128, num_heads=1, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=1)\n",
              "          (group_norm): MultiHeadLayerNorm()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ffn_norm): LayerNorm()\n",
              "        (ffn): GatedFeedForward(\n",
              "          (proj_up): Linear(in_features=128, out_features=384, bias=False)\n",
              "          (proj_down): Linear(in_features=192, out_features=128, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2-6): 5 x mLSTMBlock(\n",
              "        (xlstm_norm): LayerNorm()\n",
              "        (xlstm): mLSTMLayer(\n",
              "          (proj_up): Linear(in_features=128, out_features=512, bias=False)\n",
              "          (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
              "          (conv1d): CausalConv1d(\n",
              "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
              "          )\n",
              "          (conv_act_fn): SiLU()\n",
              "          (mlstm_cell): mLSTMCell(\n",
              "            (igate): Linear(in_features=768, out_features=4, bias=True)\n",
              "            (fgate): Linear(in_features=768, out_features=4, bias=True)\n",
              "            (outnorm): MultiHeadLayerNorm()\n",
              "          )\n",
              "          (ogate_act_fn): SiLU()\n",
              "          (proj_down): Linear(in_features=256, out_features=128, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_blocks_norm): LayerNorm()\n",
              "  )\n",
              "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "         # Calculate training accuracy for this batch\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_train += target.size(0)\n",
        "        correct_train += (predicted == target).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} \"\n",
        "          f\"Val Loss: {val_loss:.4f} \"\n",
        "          f\"Train Accuracy: {train_accuracy:.2f}% \"\n",
        "          f\"Val_Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "duUZfjXA6_jF",
        "outputId": "2cd87806-6998-4e34-8ced-bf3d7dce0d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Train Loss: 1.2972 Val Loss: 1.2365 Train Accuracy: 35.11% Val_Accuracy: 40.52%\n",
            "Epoch [2/20] Train Loss: 1.1925 Val Loss: 1.1543 Train Accuracy: 43.73% Val_Accuracy: 46.40%\n",
            "Epoch [3/20] Train Loss: 1.1071 Val Loss: 1.0777 Train Accuracy: 49.88% Val_Accuracy: 51.78%\n",
            "Epoch [4/20] Train Loss: 1.0407 Val Loss: 1.0289 Train Accuracy: 53.83% Val_Accuracy: 54.23%\n",
            "Epoch [5/20] Train Loss: 0.9969 Val Loss: 0.9931 Train Accuracy: 55.98% Val_Accuracy: 55.63%\n",
            "Epoch [6/20] Train Loss: 0.9625 Val Loss: 0.9654 Train Accuracy: 57.58% Val_Accuracy: 56.94%\n",
            "Epoch [7/20] Train Loss: 0.9339 Val Loss: 0.9431 Train Accuracy: 59.06% Val_Accuracy: 58.21%\n",
            "Epoch [8/20] Train Loss: 0.9075 Val Loss: 0.9201 Train Accuracy: 60.26% Val_Accuracy: 59.21%\n",
            "Epoch [9/20] Train Loss: 0.8833 Val Loss: 0.9002 Train Accuracy: 61.60% Val_Accuracy: 60.55%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a8f15c43518c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-39bc8d920f68>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Reshape input to match xLSTM expected dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, sequence_length, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Take the output from the last timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xlstm/xlstm_block_stack.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_blocks_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xlstm/blocks/xlstm_block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlstm_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xlstm/blocks/mlstm/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mlstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mh_tilde_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mh_tilde_state_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_tilde_state\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnable_skip\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_mlstm_conv_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xlstm/blocks/mlstm/cell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfgate_preact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgate_preact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, NH, S, 1)#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         h_state = self.backend_fn(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xlstm/blocks/mlstm/backends.py\u001b[0m in \u001b[0;36mparallel_stabilized_simple\u001b[0;34m(queries, keys, values, igate_preact, fgate_preact, lower_triangular_matrix, stabilize_rowwise, eps, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mqk_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkeys_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, NH, S, S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mC_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqk_matrix\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD_matrix\u001b[0m  \u001b[0;31m# (B, NH, S, S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_log_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, NH, S, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m# (B, NH, S, S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mC_matrix_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_matrix\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "metadata": {
        "id": "RQZSbit3_rGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Testing the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation during inference\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        y_true.extend(target.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "rQWo3VqpHluM",
        "outputId": "8d9984fb-9987-4fea-ec3c-62afaada6161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# Print the classification report\n",
        "report = classification_report(y_true, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXBAtE_UzJPY",
        "outputId": "c89647d3-ea34-4bb2-f7fd-ca2562c1abf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93        23\n",
            "           1       0.85      0.89      0.87        45\n",
            "           2       0.97      0.85      0.91        34\n",
            "           3       0.86      0.94      0.90        32\n",
            "\n",
            "    accuracy                           0.90       134\n",
            "   macro avg       0.91      0.90      0.90       134\n",
            "weighted avg       0.90      0.90      0.90       134\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "report_str = \"\"\"\n",
        "\n",
        "  precision    recall  f1-score   support\n",
        "\n",
        "           0       0.95      0.91      0.93        23\n",
        "           1       0.85      0.89      0.87        45\n",
        "           2       0.97      0.85      0.91        34\n",
        "           3       0.86      0.94      0.90        32\n",
        "\n",
        "    accuracy                           0.90       134\n",
        "   macro avg       0.91      0.90      0.90       134\n",
        "weighted avg       0.90      0.90      0.90       134\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Define a pattern to capture metrics and class names\n",
        "pattern = r\"(\\w+)\\s+([0-9.]+)\\s+([0-9.]+)\\s+([0-9.]+)\\s+([0-9]+)\"\n",
        "\n",
        "# Extract data using the pattern\n",
        "data = []\n",
        "for line in report_str.split(\"\\n\"):\n",
        "    match = re.search(pattern, line)\n",
        "    if match:\n",
        "        class_name = match.group(1)\n",
        "        precision = float(match.group(2))\n",
        "        recall = float(match.group(3))\n",
        "        f1_score = float(match.group(4))\n",
        "        support = int(match.group(5))\n",
        "        data.append([class_name, precision, recall, f1_score, support])\n",
        "\n",
        "# Convert data to a dictionary\n",
        "report_dict = {}\n",
        "for row in data:\n",
        "    class_name = row[0]\n",
        "    report_dict[class_name] = {\n",
        "        \"precision\": row[1],\n",
        "        \"recall\": row[2],\n",
        "        \"f1-score\": row[3],\n",
        "        \"support\": row[4],\n",
        "    }"
      ],
      "metadata": {
        "id": "FjBeXfCNY08Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract metrics for plotting\n",
        "categories = list(report_dict.keys())\n",
        "precision = [report_dict[c]['precision'] for c in categories]\n",
        "recall = [report_dict[c]['recall'] for c in categories]\n",
        "f1_score = [report_dict[c]['f1-score'] for c in categories]\n",
        "\n",
        "# Create bar positions\n",
        "x = np.arange(len(categories))\n",
        "width = 0.2  # Width of each bar\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1_score, width, label='F1-score')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Classification Report Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(categories)\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "qPaHmBikAU3n",
        "outputId": "61175e75-3e9a-4943-98e0-e88ac9da7323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}